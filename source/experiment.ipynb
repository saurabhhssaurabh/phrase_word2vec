{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens.tf_mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "output_model_file = os.path.join(model_dir, \"model.bin\")\n",
    "# glove_file = os.path.join(model_dir, \"glove.6B.50d.txt\")\n",
    "glove_file = os.path.join(model_dir, \"glove.txt\")\n",
    "output_glove_file = os.path.join(model_dir, \"output_glove.txt\")\n",
    "parent_tag_file = os.path.join(data_dir, \"tag_list.csv\")\n",
    "output_parent_tag_glove_file = os.path.join(model_dir, \"parent_tag_output_glove.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "this paper proposes a system called rerankeverything, which enables users to rerank search results in any search service, such as a web search engine, an e-commerce site, a hotel reservation site and so on. in conventional search services, interactions between users and services are quite limited and complicated. in addition, search functions and interactions to refine search results differ depending on the services. by using rerankeverything, users can interactively explore search results in accordance with their interests by reranking search results from various viewpoints.\n"
     ]
    }
   ],
   "source": [
    "sent = \"This paper proposes a system called RerankEverything, which enables users to rerank search results in any search service, such as a Web search engine, an e-commerce site, a hotel reservation site and so on. In conventional search services, interactions between users and services are quite limited and complicated. In addition, search functions and interactions to refine search results differ depending on the services. By using RerankEverything, users can interactively explore search results in accordance with their interests by reranking search results from various viewpoints.\".lower()\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrase_glove import PhraseGlove\n",
    "\n",
    "obj = PhraseGlove(pre_trained_glove_file=glove_file, tag_set=tag_set, output_file=output_glove_file)\n",
    "\n",
    "start_index = 0\n",
    "num_sents = 10000\n",
    "end_index = start_index + num_sents\n",
    "\n",
    "while(end_index < len(abstract_list)):\n",
    "    sent_list = abstract_list[start_index:end_index]\n",
    "    logging.info(f\"sentences from index {start_index} to index {end_index}\")\n",
    "    obj.train(sentence_list=abstract_list)\n",
    "\n",
    "    start_index = end_index\n",
    "    end_index = start_index + num_sents\n",
    "\n",
    "logging.info(\"writing embeddings to output file...\")\n",
    "obj.create_output_glove_file()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from phrase_glove import PhraseGlove\n",
    "\n",
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "glove_file = os.path.join(model_dir, \"glove.6B.50d.txt\")\n",
    "\n",
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()[:1]\n",
    "\n",
    "obj = PhraseGlove(pre_trained_glove_file=glove_file)\n",
    "obj.train(tag_set=tag_set, sentence_list=abstract_list)\n",
    "end_time = time.time()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-7efa3fc423d1>, line 19)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7efa3fc423d1>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    logging.info(f\")\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from phrase_word2vec import PhraseWord2VEC\n",
    "\n",
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "output_model_file = os.path.join(model_dir, \"model.bin\")\n",
    "\n",
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()\n",
    "\n",
    "logging.info(f\"total number of sentences: {len(abstract_list)}\")\n",
    "\n",
    "obj = PhraseWord2VEC(iter_=5)\n",
    "obj.train(tag_set=tag_set, sentence_list=abstract_list, pre_trained_model=pre_trained_model, output_model_file=output_model_file)\n",
    "end_time = time.time()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Filter (signal processing'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "string_1 = 'Filter (signal processing)'\n",
    "string_1[:string_1.find('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "None\n{'key1': 1, 'key2': 3, 'key3': 4}\n"
     ]
    }
   ],
   "source": [
    "dict_1 = {'key1': 1, 'key2': 2}\n",
    "dict_2 = {'key2': 3, 'key3': 4}\n",
    "\n",
    "test = dict_1.update(dict_2)\n",
    "print(test)\n",
    "print(dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,1))\n",
    "matrix = cv.fit_transform([sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict_keys(['this', 'paper', 'proposes', 'system', 'called', 'rerankeverything', 'which', 'enables', 'users', 'to', 'rerank', 'search', 'results', 'in', 'any', 'service', 'such', 'as', 'web', 'engine', 'an', 'commerce', 'site', 'hotel', 'reservation', 'and', 'so', 'on', 'conventional', 'services', 'interactions', 'between', 'are', 'quite', 'limited', 'complicated', 'addition', 'functions', 'refine', 'differ', 'depending', 'the', 'by', 'using', 'can', 'interactively', 'explore', 'accordance', 'with', 'their', 'interests', 'reranking', 'from', 'various', 'viewpoints'])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "cv.vocabulary_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = dict(sorted(cv.vocabulary_.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "human–computer_interaction => None\ncomputer_engineering => None\nreal-time_computing => None\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = ' Human–computer interaction'\n",
    "tag = tag.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "index = tag.find('-')\n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index > -1:\n",
    "    tag = tag.replace('-', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'human–computer_interaction'"
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tag.replace(' ', '_')"
   ]
  }
 ]
}