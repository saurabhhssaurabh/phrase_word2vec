{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('general_python3_env': venv)",
   "metadata": {
    "interpreter": {
     "hash": "60d1513e2307e7270bf9e3e8b5a9805c78a7f313e74e8e050a7b70485d276108"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import logging\n",
    "import datetime\n",
    "import csv\n",
    "import numpy as np\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens.tf_mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "output_model_file = os.path.join(model_dir, \"model.bin\")\n",
    "glove_file = os.path.join(model_dir, \"glove.6B\", \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrase_word2vec import PhraseWord2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = PhraseWord2VEC()\n",
    "# obj.train(tag_set, abstract_list, pre_trained_model, output_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:pre processing input sentences \n"
     ]
    }
   ],
   "source": [
    "pre_processed_sent = obj.pre_process(tag_set, abstract_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = glove2dict(glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "oov_word_set = set()\n",
    "\n",
    "for list_ in pre_processed_sent:\n",
    "    for word in list_:\n",
    "        word_list.append(word)\n",
    "        if word not in embedding_dict.keys():\n",
    "            oov_word_set.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Iteration 210: loss: 0.00010739189747255296"
     ]
    }
   ],
   "source": [
    "mittens_model = Mittens(n=50, max_iter=1000)\n",
    "new_embeddings = mittens_model.fit(\n",
    "    coocc_ar,\n",
    "    vocab=list(oov_word_set),\n",
    "    initial_embedding_dict= embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.csv', 'a', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=' ',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow([list(oov_word_set)[1]] + list(new_embeddings[1]))"
   ]
  }
 ]
}