{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "60d1513e2307e7270bf9e3e8b5a9805c78a7f313e74e8e050a7b70485d276108"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor\n",
    "import pandas as pd\n",
    "import os\n",
    "import gensim\n",
    "import logging\n",
    "import time\n",
    "import csv\n",
    "import numpy as np\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens.tf_mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "output_model_file = os.path.join(model_dir, \"model.bin\")\n",
    "glove_file = os.path.join(model_dir, \"glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phrase_glove import PhraseGlove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:generating word vector dictionary\n"
     ]
    }
   ],
   "source": [
    "obj = PhraseGlove(pre_trained_glove_file=glove_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:pre processing input sentences \n",
      "INFO:root:number of oov words 5\n",
      "INFO:root:generating cooccurrence matrix...\n",
      "INFO:root:starting training...\n",
      "Iteration 150: loss: 0.00010294899402651936INFO:root:writing new embedding to file...\n",
      "total time taken: 190.31462407112122\n"
     ]
    }
   ],
   "source": [
    "obj.train(tag_set=tag_set, sentence_list=abstract_list)\n",
    "end_time = time.time()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from phrase_glove import PhraseGlove\n",
    "\n",
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "glove_file = os.path.join(model_dir, \"glove.6B.50d.txt\")\n",
    "\n",
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()[:1]\n",
    "\n",
    "obj = PhraseGlove(pre_trained_glove_file=glove_file)\n",
    "obj.train(tag_set=tag_set, sentence_list=abstract_list)\n",
    "end_time = time.time()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-7efa3fc423d1>, line 19)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-7efa3fc423d1>\"\u001b[1;36m, line \u001b[1;32m19\u001b[0m\n\u001b[1;33m    logging.info(f\")\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from phrase_word2vec import PhraseWord2VEC\n",
    "\n",
    "root_dir = os.path.abspath(os.pardir)\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "model_dir = os.path.join(root_dir, \"model\")\n",
    "tag_table_file = os.path.join(data_dir, \"paper_tag_table.csv\")\n",
    "paper_table_file = os.path.join(data_dir, \"paper_table.csv\")\n",
    "pre_trained_model = os.path.join(model_dir, \"GoogleNews-vectors-negative300.bin\")\n",
    "output_model_file = os.path.join(model_dir, \"model.bin\")\n",
    "\n",
    "start_time = time.time()\n",
    "tag_set = set(pd.read_csv(tag_table_file)['tag_text'].dropna().tolist())\n",
    "abstract_list = pd.read_csv(paper_table_file)['abstract'].dropna().tolist()\n",
    "\n",
    "logging.info(f\"total number of sentences: {len(abstract_list)}\")\n",
    "\n",
    "obj = PhraseWord2VEC(iter_=5)\n",
    "obj.train(tag_set=tag_set, sentence_list=abstract_list, pre_trained_model=pre_trained_model, output_model_file=output_model_file)\n",
    "end_time = time.time()\n",
    "print(f\"total time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Filter (signal processing'"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "string_1 = 'Filter (signal processing)'\n",
    "string_1[:string_1.find('#')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}